import argparse
import time
from pathlib import Path
from typing import Optional, Union, List, Tuple

import numpy as np

from svecalign.utils.audio_utils import SAMPLE_RATE, load_waveform, save_waveform
from svecalign.utils.log_utils import logging

EMBED_MODEL_TYPES = ["speech_laser", "sonar"]

logger = logging.getLogger(__name__)


def add_embed_args(parser: argparse.ArgumentParser) -> argparse.ArgumentParser:
    parser.add_argument(
        "--embed_model_type", choices=EMBED_MODEL_TYPES,
        default="speech_laser"
    )
    parser.add_argument(
        "--embed_fp32", action="store_true", default=False,
        help="whether to save the embeddings in fp32. default is fp16."
    )
    # SpeechLASER setups
    parser.add_argument(
        "--sl_ckpt_dir", type=str, default=None,
        help="the dir to all speech laser models."
    )
    parser.add_argument(
        "--sl_ckpt_name", type=str, default=None,
        help="which SpeechLASER to use. e.g., `english.pt`"
    )
    parser.add_argument(
        "--max_tokens", type=int,
        default=150 * SAMPLE_RATE,  # 150s
        help="in number of SAMPLES"
    )
    # SONAR setups
    parser.add_argument(
        "--sonar_name", type=str,
        # see https://github.com/facebookresearch/SONAR/blob/main/sonar/cards/sonar_speech_encoder.yaml
        help="name of the sonar model."
    )
    parser.add_argument(
        "--batch_size", type=int,
        help="number of audios per batch."
    )
    parser.add_argument(
        "--n_proc", type=int, default=1
    )
    parser.add_argument(
        "--compile_sonar", action="store_true", default=False,
        help="Will call torch.compile() if true."
    )
    return parser


def load_embed_model(
        embed_model_type: str,
        sl_ckpt_dir: Optional[str] = None, sl_ckpt_name: Optional[str] = None, max_tokens: Optional[int] = None,
        sonar_name: Optional[str] = None, sonar_fp16: bool = False, compile_sonar: bool = False
):
    if embed_model_type == "speech_laser":
        assert sl_ckpt_dir is not None and sl_ckpt_name is not None and max_tokens is not None
        import fairseq  # noqa
        import fairseq.checkpoint_utils  # noqa
        from stopes.modules.preprocess.wav2vec_laser_speech_encoder import (  # noqa
            LaserFileAudioEncoder,  # noqa
        )
        embed_model = LaserFileAudioEncoder(
            Path(sl_ckpt_dir),
            sl_ckpt_name,
            max_tokens,
            logger,
        )
    elif embed_model_type == "sonar":
        assert sonar_name is not None
        import torch
        from sonar.inference_pipelines.speech import SpeechToEmbeddingModelPipeline  # noqa
        sonar_dtype = torch.float16 if sonar_fp16 else torch.float32  # the latest ver should've fixed the fp16 issue
        logger.info(
            f"Will use {sonar_dtype} for SONAR. If there is an error for fp16, please install the latest version."
        )
        embed_model = SpeechToEmbeddingModelPipeline(
            encoder=sonar_name,
            device=torch.device("cuda:0"),
            fbank_dtype=sonar_dtype
        )
        if compile_sonar:
            # ref: https://github.com/facebookresearch/SONAR/issues/51#issuecomment-2730050529
            logger.info(f"Start compiling SONAR model...")
            s = time.time()
            embed_model.model = torch.compile(embed_model.model)
            e = time.time()
            logger.info(f"Compilation took {e - s:.2f} sec")
    else:
        raise NotImplementedError(f"Unsupported model {embed_model_type}")
    return embed_model


def embed_to_file(embed_model, embed_model_type: str,
                  in_dir: Path, out_path: Path,
                  fp16: bool = True,
                  batch_size: Optional[int] = None,
                  n_proc: int = 1):
    """
    Generate embeddings using the given embed_model. Could be either speech laser or sonar.
    in_dir: the input data dir, should contain a metadata file `in_dir/tsv`,
        which can be generated by `save_segment_audio_and_tsv`
    out_path: the file to store all embeddings.
    batch_size and n_proc: for SONAR.
    """
    if embed_model_type == "speech_laser":
        embed_model.encode_file(
            (in_dir / "tsv").as_posix(),
            out_path.as_posix(),
            fp16=fp16
        )
    elif embed_model_type == "sonar":
        assert batch_size is not None and batch_size > 0
        embedding = embed_model.predict(
            input=convert_audio_frame_tsv_to_paths(in_dir / "tsv"),
            batch_size=batch_size,
            n_parallel=n_proc,
            progress_bar=False
        ).cpu().numpy()
        with open(out_path, "wb") as fp:
            np.ascontiguousarray(
                embedding,
                dtype=np.float16 if fp16 else np.float32
            ).tofile(fp)
    else:
        raise NotImplementedError(f"Unsupported model {embed_model_type}")


def save_segment_audio_and_tsv(
        out_dir: Union[str, Path],
        wave_path: Union[str, Path],
        segments: List[Tuple[int, int]],
        wav_dir_name: str = "wavs",
        tsv_file_name: str = "tsv",
        ext: str = "wav"
):
    """
    Extract all segments from `wave_path`.
    Save the segmented audios to `out_dir/wav_dir_name`.
    Generate a corresponding tsv manifest file `out_dir/tsv_file_name`.
    """
    # prepare for dir
    if isinstance(out_dir, str):
        out_dir = Path(out_dir)

    wav_out = out_dir / wav_dir_name
    wav_out.mkdir(parents=True, exist_ok=True)

    tsv_out = out_dir / tsv_file_name

    # load audio
    if isinstance(wave_path, Path):
        wave_path = wave_path.as_posix()

    with open(tsv_out, mode="w") as fp:
        fp.write(f"{wav_out.as_posix()}\n")

        for i, (s, e) in enumerate(segments):
            wav = load_waveform(
                wave_path, start=s, end=e
            )
            save_waveform(
                wav, target=wav_out / f"{i}.{ext}", overwrite_wav=True
            )
            fp.write(
                f"{i}.{ext}\t{e - s}\n"
            )


def convert_audio_frame_tsv_to_paths(tsv_path: Union[str, Path]) -> List[str]:
    """
    Convert the metadata tsv file into a list of audio paths, in order to match SONAR's input format.
    :param tsv_path: format:
    /path/to/audio/dir
    a.wav\t123
    b.wav\t123
    """
    res = []
    with open(tsv_path) as fp:
        base_dir = Path(fp.readline().strip())
        for line in fp:
            sub_path, _ = line.strip().split("\t")
            res.append(
                (base_dir / sub_path).as_posix()
            )
    return res
